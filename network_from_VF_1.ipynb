{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import os\n",
    "import graph_tool.all as gt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(p_group=False):\n",
    "    # load data frame into memory\n",
    "    df = pd.read_csv('./VF_data/region_niedersachsen_tapas_modell.csv')\n",
    "\n",
    "    # sort data by person and by start time\n",
    "    df = df.sort_values(['p_id', 'start_time_min'], ignore_index=True)\n",
    "\n",
    "    # normalize p_id to be index like only the first time the cell is executed\n",
    "    if not df['p_id'].iloc[0] == 0:\n",
    "        df['p_id'] = df['p_id'] - 100_000_000\n",
    "\n",
    "    # compute absolute value of loc_id_end and loc_id_start\n",
    "    df['loc_id_end'] = df['loc_id_end'].abs()\n",
    "    df['loc_id_start'] = df['loc_id_start'].abs()\n",
    "\n",
    "    # compute activity end time\n",
    "    df['activity_end_min'] = df['activity_start_min'] + df['activity_duration_min']\n",
    "\n",
    "    # Downcast times\n",
    "    # Select important coloumns\n",
    "    if p_group:\n",
    "        df = df.astype({'p_id': 'int32', 'loc_id_end': 'int32', 'activity_start_min': 'int16', 'activity_end_min': 'int16',\n",
    "                        'p_group': 'int8'})\n",
    "        df = df[['p_id', 'loc_id_end', 'activity_start_min', 'activity_end_min', 'p_group']]\n",
    "    else:\n",
    "        df = df.astype({'p_id': 'int32', 'loc_id_end': 'int32', 'activity_start_min': 'int16', 'activity_end_min': 'int16'})\n",
    "        df = df[['p_id', 'loc_id_end', 'activity_start_min', 'activity_end_min']]\n",
    "\n",
    "    # Save to parquet\n",
    "    df.to_parquet('./VF_data/rns_data.parquet')\n",
    "\n",
    "csv_to_parquet(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./VF_data/rns_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9012236 entries, 0 to 9012235\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Dtype\n",
      "---  ------              -----\n",
      " 0   p_id                int32\n",
      " 1   loc_id_end          int32\n",
      " 2   activity_start_min  int16\n",
      " 3   activity_end_min    int16\n",
      " 4   p_group             int8 \n",
      "dtypes: int16(2), int32(2), int8(1)\n",
      "memory usage: 111.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contacts_vect(start_times, end_times, event_ids, loc_id_end):\n",
    "    # Broadcast the start_time and end_time arrays for comparison with each other\n",
    "    overlap_start = np.maximum.outer(start_times, start_times)\n",
    "    overlap_end = np.minimum.outer(end_times, end_times)\n",
    "\n",
    "    # Calculate the overlap duration matrix using NumPy\n",
    "    overlap_durations = np.maximum(overlap_end - overlap_start, np.zeros(shape=overlap_start.shape)).astype('uint16')\n",
    "    \n",
    "    # Set diagonal elements to zero (overlap of an event with itself and double counting)\n",
    "    overlap_durations = np.triu(overlap_durations, 1)\n",
    "\n",
    "    # Extract contact rows, cols\n",
    "    rows, cols = np.where(overlap_durations > 0)\n",
    "    p_A = event_ids[rows]\n",
    "\n",
    "    # Save contacts to new DataFrame\n",
    "    contact_data = {'p_A': p_A,'p_B': event_ids[cols], \n",
    "                    'start_of_contact': overlap_start[rows, cols],\n",
    "                    'end_of_contact': overlap_end[rows, cols],\n",
    "                    'loc_id': np.repeat(loc_id_end, len(p_A)).astype('int32')}\n",
    "\n",
    "    return pd.DataFrame(contact_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped\n",
      "Created 147 chunks of size 10000\n"
     ]
    }
   ],
   "source": [
    "# Setup loop over groups\n",
    "# Group by loc_id_end\n",
    "grouped = df.groupby('loc_id_end')\n",
    "group_keys = np.array(list(grouped.groups.keys()))\n",
    "print('Grouped')\n",
    "\n",
    "# Shuffle groups\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(group_keys)\n",
    "\n",
    "chunk_size = 10_000\n",
    "N = len(group_keys)\n",
    "\n",
    "# Create padding, transform into chunks\n",
    "pad = (0, chunk_size * np.ceil(N / chunk_size).astype('int') - N)\n",
    "group_keys = np.pad(group_keys, pad_width=pad, constant_values=(0, 0))\n",
    "group_keys = group_keys.reshape(-1, chunk_size)\n",
    "N = len(group_keys)\n",
    "print(f'Created {N} chunks of size {chunk_size}')\n",
    "\n",
    "# Fast group acces\n",
    "groups = dict(list(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start extracting contacts\n",
      "Finished chunk 5/146 in 15.39s\n",
      "Finished chunk 0/146 in 15.57s\n",
      "Finished chunk 4/146 in 15.63s\n",
      "Finished chunk 8/146 in 15.63s\n",
      "Finished chunk 9/146 in 15.9s\n",
      "Finished chunk 3/146 in 15.97s\n",
      "Finished chunk 2/146 in 16.58s\n",
      "Finished chunk 7/146 in 18.16s\n",
      "Finished chunk 1/146 in 18.49s\n",
      "Finished chunk 13/146 in 4.52s\n",
      "Finished chunk 11/146 in 4.69s\n",
      "Finished chunk 6/146 in 20.4s\n",
      "Finished chunk 14/146 in 4.89s\n",
      "Finished chunk 10/146 in 5.83s\n",
      "Finished chunk 16/146 in 4.99s\n",
      "Finished chunk 15/146 in 5.87s\n",
      "Finished chunk 17/146 in 4.66s\n",
      "Finished chunk 18/146 in 5.21s\n",
      "Finished chunk 21/146 in 4.82s\n",
      "Finished chunk 19/146 in 5.16s\n",
      "Finished chunk 23/146 in 4.66s\n",
      "Finished chunk 22/146 in 6.75s\n",
      "Finished chunk 20/146 in 7.48s\n",
      "Finished chunk 26/146 in 5.12s\n",
      "Finished chunk 25/146 in 6.36s\n",
      "Finished chunk 27/146 in 4.54s\n",
      "Finished chunk 29/146 in 4.68s\n",
      "Finished chunk 30/146 in 4.61s\n",
      "Finished chunk 28/146 in 6.97s\n",
      "Finished chunk 31/146 in 5.55s\n",
      "Finished chunk 33/146 in 5.53s\n",
      "Finished chunk 34/146 in 5.6s\n",
      "Finished chunk 37/146 in 4.63s\n",
      "Finished chunk 12/146 in 19.62s\n",
      "Finished chunk 32/146 in 8.61s\n",
      "Finished chunk 35/146 in 8.29s\n",
      "Finished chunk 36/146 in 8.04s\n",
      "Finished chunk 41/146 in 4.94s\n",
      "Finished chunk 40/146 in 5.39s\n",
      "Finished chunk 42/146 in 5.91s\n",
      "Finished chunk 45/146 in 4.78s\n",
      "Finished chunk 38/146 in 9.44s\n",
      "Finished chunk 43/146 in 6.47s\n",
      "Finished chunk 44/146 in 6.3s\n",
      "Finished chunk 47/146 in 4.44s\n",
      "Finished chunk 46/146 in 5.17s\n",
      "Finished chunk 39/146 in 10.69s\n",
      "Finished chunk 49/146 in 4.46s\n",
      "Finished chunk 48/146 in 6.66s\n",
      "Finished chunk 50/146 in 4.76s\n",
      "Finished chunk 51/146 in 4.78s\n",
      "Finished chunk 55/146 in 4.51s\n",
      "Finished chunk 54/146 in 4.55s\n",
      "Finished chunk 52/146 in 7.72s\n",
      "Finished chunk 56/146 in 5.95s\n",
      "Finished chunk 59/146 in 5.82s\n",
      "Finished chunk 53/146 in 9.46s\n",
      "Finished chunk 60/146 in 6.65s\n",
      "Finished chunk 58/146 in 8.86s\n",
      "Finished chunk 64/146 in 4.97s\n",
      "Finished chunk 63/146 in 5.67s\n",
      "Finished chunk 66/146 in 4.53s\n",
      "Finished chunk 67/146 in 4.3s\n",
      "Finished chunk 70/146 in 4.48s\n",
      "Finished chunk 62/146 in 15.81s\n",
      "Finished chunk 61/146 in 18.62s\n",
      "Finished chunk 65/146 in 15.74s\n",
      "Finished chunk 73/146 in 8.45s\n",
      "Finished chunk 74/146 in 4.87s\n",
      "Finished chunk 57/146 in 24.29s\n",
      "Finished chunk 68/146 in 16.25s\n",
      "Finished chunk 69/146 in 17.22sFinished chunk 75/146 in 5.59s\n",
      "\n",
      "Finished chunk 76/146 in 4.64s\n",
      "Finished chunk 71/146 in 15.92s\n",
      "Finished chunk 72/146 in 17.13s\n",
      "Finished chunk 79/146 in 5.13s\n",
      "Finished chunk 24/146 in 55.0s\n",
      "Finished chunk 84/146 in 5.23s\n",
      "Finished chunk 78/146 in 10.99s\n",
      "Finished chunk 81/146 in 7.68s\n",
      "Finished chunk 86/146 in 5.21s\n",
      "Finished chunk 83/146 in 8.62s\n",
      "Finished chunk 87/146 in 6.16s\n",
      "Finished chunk 89/146 in 4.34s\n",
      "Finished chunk 82/146 in 12.15s\n",
      "Finished chunk 85/146 in 9.76s\n",
      "Finished chunk 91/146 in 4.5s\n",
      "Finished chunk 80/146 in 15.75s\n",
      "Finished chunk 88/146 in 8.84s\n",
      "Finished chunk 92/146 in 5.89s\n",
      "Finished chunk 90/146 in 8.78s\n",
      "Finished chunk 96/146 in 4.15s\n",
      "Finished chunk 97/146 in 4.84s\n",
      "Finished chunk 77/146 in 21.69s\n",
      "Finished chunk 93/146 in 7.32s\n",
      "Finished chunk 95/146 in 6.49s\n",
      "Finished chunk 100/146 in 4.41s\n",
      "Finished chunk 99/146 in 5.78s\n",
      "Finished chunk 94/146 in 10.02s\n",
      "Finished chunk 101/146 in 5.76s\n",
      "Finished chunk 102/146 in 5.89s\n",
      "Finished chunk 104/146 in 4.65s\n",
      "Finished chunk 106/146 in 4.34s\n",
      "Finished chunk 105/146 in 4.88s\n",
      "Finished chunk 98/146 in 8.75s\n",
      "Finished chunk 107/146 in 6.77s\n",
      "Finished chunk 111/146 in 4.48s\n",
      "Finished chunk 112/146 in 4.44s\n",
      "Finished chunk 110/146 in 4.7s\n",
      "Finished chunk 114/146 in 5.77s\n",
      "Finished chunk 103/146 in 11.46s\n",
      "Finished chunk 115/146 in 6.27s\n",
      "Finished chunk 109/146 in 8.69s\n",
      "Finished chunk 116/146 in 4.74s\n",
      "Finished chunk 118/146 in 6.12s\n",
      "Finished chunk 119/146 in 6.15s\n",
      "Finished chunk 122/146 in 4.55s\n",
      "Finished chunk 113/146 in 11.55s\n",
      "Finished chunk 117/146 in 8.48s\n",
      "Finished chunk 123/146 in 5.56s\n",
      "Finished chunk 124/146 in 5.73s\n",
      "Finished chunk 121/146 in 9.18s\n",
      "Finished chunk 126/146 in 5.53s\n",
      "Finished chunk 128/146 in 4.7s\n",
      "Finished chunk 108/146 in 19.28s\n",
      "Finished chunk 125/146 in 7.34s\n",
      "Finished chunk 129/146 in 6.93s\n",
      "Finished chunk 133/146 in 4.75s\n",
      "Finished chunk 135/146 in 4.23s\n",
      "Finished chunk 131/146 in 7.58s\n",
      "Finished chunk 130/146 in 8.09s\n",
      "Finished chunk 134/146 in 5.37s\n",
      "Finished chunk 136/146 in 4.77s\n",
      "Finished chunk 132/146 in 7.97s\n",
      "Finished chunk 137/146 in 5.41s\n",
      "Finished chunk 138/146 in 4.4s\n",
      "Finished chunk 120/146 in 19.66s\n",
      "Finished chunk 139/146 in 4.72s\n",
      "Finished chunk 127/146 in 14.69s\n",
      "Finished chunk 141/146 in 5.17s\n",
      "Finished chunk 143/146 in 4.37s\n",
      "Finished chunk 142/146 in 5.3s\n",
      "Finished chunk 144/146 in 4.78s\n",
      "Finished chunk 145/146 in 4.61s\n",
      "Finished chunk 140/146 in 19.5s\n",
      "Finished chunk 146/146 in 12.53s\n",
      "Concate results into single DataFrame\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "Export to parquet\n"
     ]
    }
   ],
   "source": [
    "print('Start extracting contacts')\n",
    "def contacts_per_location(grp, loc_id):\n",
    "    # Extracts all contacts from a certain location\n",
    "    # print(grp.shape[0], grp.name)\n",
    "\n",
    "    return get_contacts_vect(grp.activity_start_min.values, grp.activity_end_min.values, grp.p_id.values, loc_id)\n",
    "\n",
    "def contacts_per_location_wrapper(chunk_ind):\n",
    "    start_time = time.perf_counter()\n",
    "    chunk = group_keys[chunk_ind]\n",
    "    df_temp = pd.concat([contacts_per_location(groups[key], key) for key in chunk], ignore_index=True, sort=False)\n",
    "    df_temp.to_parquet(f'./parquets/rns/rns_contacts_chunk{str(chunk_ind).zfill(3)}.parquet')\n",
    "    dt = time.perf_counter() -  start_time\n",
    "    print(f'Finished chunk {chunk_ind}/{N-1} in {round(dt, 2)}s')\n",
    "    return chunk_ind\n",
    "\n",
    "\n",
    "with Pool(processes=10) as pool:\n",
    "    results = pool.imap_unordered(contacts_per_location_wrapper, range(N - 1))\n",
    "\n",
    "    for res in results:\n",
    "        # Remove padding from final chunk\n",
    "        if res == N - 2:\n",
    "            start_time = time.perf_counter()\n",
    "            chunk_final = group_keys[-1]\n",
    "            chunk_final = chunk_final[chunk_final > 0]\n",
    "\n",
    "            df_temp = pd.concat([contacts_per_location(groups[key], key) for key in chunk_final], ignore_index=True, sort=False)\n",
    "            df_temp.to_parquet(f'./parquets/rns/rns_contacts_chunk{str(N - 1).zfill(3)}.parquet')\n",
    "            dt = time.perf_counter() -  start_time\n",
    "            print(f'Finished chunk {N-1}/{N-1} in {round(dt, 2)}s')\n",
    "\n",
    "print('Concate results into single DataFrame')\n",
    "base_dir = './parquets/rns/'\n",
    "files = os.listdir(base_dir)\n",
    "\n",
    "all_dfs = []\n",
    "for i, file in enumerate(files):\n",
    "    print(i)\n",
    "    all_dfs.append(pd.read_parquet(base_dir + file))\n",
    "    \n",
    "df_contacts = pd.concat(all_dfs)\n",
    "\n",
    "print('Export to parquet')\n",
    "df_contacts.to_parquet('./parquets/rns_contacts_full_1.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network genertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load parquet file\n",
      "Add edges\n"
     ]
    }
   ],
   "source": [
    "def full_network_generation(edge_path='./parquets/rns_contacts_full_1.parquet'):\n",
    "    # It's faster to create a new Graph from the edges contained in the parquet file,\n",
    "    # than to save the graph as a gt-file and reloade it.\n",
    "\n",
    "    print('Load parquet file')\n",
    "    edge_list = pd.read_parquet(edge_path, columns=['p_A', 'p_B']).values\n",
    "    print('Add edges')\n",
    "    G = gt.Graph(directed=False)\n",
    "    G.add_edge_list(edge_list)\n",
    "\n",
    "    return G\n",
    "\n",
    "G = full_network_generation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not used anymore\n",
    "Maybe interesting for later\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandemic_networks_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
